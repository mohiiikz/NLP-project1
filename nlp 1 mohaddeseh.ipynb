{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6782b0d0-aa9b-4f6f-9e58-1bde99f219b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e966f5-e498-49df-a8f2-2e027d9d1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af35db21-3470-4b25-a47d-0d74d698db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, FreqDist, download\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB  # Add this import\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, FreqDist, download\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import wikipediaapi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef10d6d7-aa73-43d4-848a-deefc3b53699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Naïve Bayes): 1.0\n",
      "Precision (Naïve Bayes): 1.0\n",
      "Recall (Naïve Bayes): 1.0\n",
      "The content of the website \"https://en.wikipedia.org/wiki/Cinematography\" is non-geographical according to Logistic Regression with TF-IDF.\n"
     ]
    }
   ],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia('english')\n",
    "\n",
    "# Function to retrieve a text from a Wikipedia page\n",
    "def get_wikipedia_text(page_title):\n",
    "    page = wiki_wiki.page(page_title)\n",
    "    \n",
    "    if not page.exists():\n",
    "        return None\n",
    "    \n",
    "    return page.text\n",
    "\n",
    "# Function to retrieve text from a website\n",
    "def get_website_text(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')  # You may need to adjust this based on the HTML structure of the website\n",
    "        text = ' '.join([paragraph.get_text() for paragraph in paragraphs])\n",
    "        return text\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_keywords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "def extract_nouns(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = pos_tag(words)\n",
    "    nouns = [word.lower() for word, pos in tagged_words if pos.startswith('N') and word.lower() not in stop_words and word.isalnum()]\n",
    "    return nouns\n",
    "\n",
    "def extract_top_nouns(topics, num_top_nouns=10):\n",
    "    all_nouns = []\n",
    "\n",
    "    for topic in topics:\n",
    "        text = get_wikipedia_text(topic)\n",
    "        if text:\n",
    "            all_nouns.extend(extract_nouns(text))\n",
    "\n",
    "    nouns_freq_dist = FreqDist(all_nouns)\n",
    "    top_nouns = [word for word, _ in nouns_freq_dist.most_common(num_top_nouns)]\n",
    "\n",
    "    return top_nouns\n",
    "\n",
    "# Sample annotated keywords for geographical and non-geographical topics\n",
    "geographical_topics = ['geography', 'GIS', 'map', 'Geoprocessing', 'surface','sun','wind', 'ocean','water']\n",
    "non_geographical_topics = ['programming', 'technology', 'history', 'medical', 'estimates','behave','physic', 'economy']\n",
    "\n",
    "top_geographical_keywords = extract_top_nouns(geographical_topics, num_top_nouns=10)\n",
    "top_non_geographical_keywords = extract_top_nouns(non_geographical_topics, num_top_nouns=10)\n",
    "\n",
    "all_topics = geographical_topics + non_geographical_topics\n",
    "all_docs = []\n",
    "all_labels = []\n",
    "\n",
    "for topic in all_topics:\n",
    "    text = get_wikipedia_text(topic)\n",
    "    if text:\n",
    "        nouns = extract_nouns(text)\n",
    "        all_docs.append(\" \".join(nouns))\n",
    "        all_labels.append(1 if topic in geographical_topics else 0)\n",
    "\n",
    "all_top_keywords = top_geographical_keywords + top_non_geographical_keywords\n",
    "vectorizer = TfidfVectorizer(vocabulary=all_top_keywords)\n",
    "X = vectorizer.fit_transform(all_docs)\n",
    "y = all_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression classifier\n",
    "logistic_classifier = LogisticRegression()\n",
    "logistic_classifier.fit(X_train, y_train)\n",
    "logistic_predictions = logistic_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_logistic = accuracy_score(y_test, logistic_predictions)\n",
    "\n",
    "# Replace the URL with the website you want to classify\n",
    "website_url = \"https://en.wikipedia.org/wiki/Cinematography\"\n",
    "website_text = get_website_text(website_url)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "naive_bayes_predictions = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy, precision, and recall\n",
    "accuracy_naive_bayes = accuracy_score(y_test, naive_bayes_predictions)\n",
    "precision_naive_bayes = precision_score(y_test, naive_bayes_predictions)\n",
    "recall_naive_bayes = recall_score(y_test, naive_bayes_predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy (Naïve Bayes): {accuracy_naive_bayes}\")\n",
    "print(f\"Precision (Naïve Bayes): {precision_naive_bayes}\")\n",
    "print(f\"Recall (Naïve Bayes): {recall_naive_bayes}\")\n",
    "\n",
    "if website_text:\n",
    "    # Vectorize the website text using TF-IDF\n",
    "    website_vectorized = vectorizer.transform([\" \".join(extract_nouns(website_text))])\n",
    "\n",
    "    # Logistic Regression prediction using TF-IDF\n",
    "    logistic_website_prediction = logistic_classifier.predict(website_vectorized)\n",
    "\n",
    "    if logistic_website_prediction == 1:\n",
    "        print(f'The content of the website \"{website_url}\" is geographical according to Logistic Regression with TF-IDF.')\n",
    "    else:\n",
    "        print(f'The content of the website \"{website_url}\" is non-geographical according to Logistic Regression with TF-IDF.')\n",
    "else:\n",
    "    print(f'Unable to fetch text from the website \"{website_url}\". Please check the URL or try another website.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd14efac-9fa0-4c6f-9d58-8ee20940924f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
